{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import mnist_inference\n",
    "import Layer2Model\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100 \n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30001\n",
    "MOVING_AVERAGE_DECAY = 0.99 \n",
    "MODEL_SAVE_PATH = \"model/{}\".format(pd.Timestamp.now())[:16]\n",
    "MODEL_NAME = \"tianchi_model\"\n",
    "two_layers_model_fileName = '2HiddenLayer'\n",
    "TENSORBOARD_LOG = 'tensor_board'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def startTrain(trainX, trainY, model_path, model_name, model):\n",
    "    dataSize = len(trainY)\n",
    "    \n",
    "    with tf.device('/device:GPU:0'):\n",
    "    # 定义输入输出placeholder。\n",
    "        x = tf.placeholder(tf.float32, [None, model.INPUT_NODE], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, model.OUTPUT_NODE], name='y-input')\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "        y = model.inference(x, regularizer)\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "#     cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "#     cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "        beginLoss = tf.sqrt(tf.reduce_mean(tf.pow(tf.subtract(y, y_), 2)))\n",
    "#     loss = beginLoss + tf.add_n(tf.get_collection('losses'))\n",
    "        loss = beginLoss\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            LEARNING_RATE_BASE,\n",
    "            global_step,\n",
    "            dataSize / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "            staircase=True)\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "        with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    config = tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "    with tf.Session(config = config) as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        head = 0\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            tail = head+BATCH_SIZE\n",
    "            if tail > dataSize:\n",
    "                xs = np.concatenate((trainX[head: BATCH_SIZE], trainX[0: tail-BATCH_SIZE]))\n",
    "                ys = np.concatenate((trainY[head: BATCH_SIZE], trainY[0: tail-BATCH_SIZE]))\n",
    "                head = tail - BATCH_SIZE\n",
    "            else:\n",
    "                xs, ys = trainX[head: head+BATCH_SIZE-1], trainY[head: head+BATCH_SIZE-1]\n",
    "                head = tail\n",
    "            \n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After\", step,\" training step(s), loss on training batch is \", loss_value)\n",
    "            if i % 10000 == 0:\n",
    "                saver.save(sess, os.path.join(model_path, model_name), global_step=global_step)\n",
    "                testLoss = sess.run([loss], feed_dict={x: testX, y_: testY})\n",
    "                print(\"After\", step,\" training step(s), loss on test set is \", testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# handle data, produce train and test input\n",
    "\n",
    "df = pd.read_csv('../CleanData/Max_trainDataAfterClean.csv')\n",
    "# random shift the df\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "normalizeColumns = ['compartment','TR','displacement','price_level','power','level_id',\n",
    "                    'cylinder_number','engine_torque','car_length','car_height','car_width','total_quality','equipment_quality',\n",
    "                    'rated_passenger','wheelbase','front_track','rear_track']\n",
    "leftDf = df.drop(normalizeColumns, axis =1 ).drop(['sale_quantity'], axis = 1)\n",
    "\n",
    "normalizeDf = df[normalizeColumns]\n",
    "normalizeDf = (normalizeDf-normalizeDf.min())/(normalizeDf.max()-normalizeDf.min())\n",
    "inputDf = pd.concat([leftDf, normalizeDf], axis = 1)\n",
    "inputX = inputDf.values\n",
    "resultArray = df['sale_quantity'].values\n",
    "inputY = resultArray.reshape((len(resultArray),1))\n",
    "trainX = inputX[0:18000]\n",
    "trainY = inputY[0:18000]\n",
    "testX = inputX[18000:]\n",
    "testY = inputY[18000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def continueTrain(trainX, trainY, sess, continue_steps):\n",
    "    head = 0\n",
    "    for i in range(continue_steps):\n",
    "        tail = head+BATCH_SIZE\n",
    "        if tail > dataSize:\n",
    "            xs = np.concatenate((trainX[head: BATCH_SIZE], trainX[0: tail-BATCH_SIZE]))\n",
    "            ys = np.concatenate((trainY[head: BATCH_SIZE], trainY[0: tail-BATCH_SIZE]))\n",
    "            head = tail - BATCH_SIZE\n",
    "        else:\n",
    "            xs, ys = trainX[head: head+BATCH_SIZE-1], trainY[head: head+BATCH_SIZE-1]\n",
    "            head = tail\n",
    "            \n",
    "        _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After\", step,\" training step(s), loss on training batch is \", loss_value)\n",
    "        if i % 10000 == 0:\n",
    "            saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "            testLoss = sess.run([loss], feed_dict={x: testX, y_: testY})\n",
    "            print(\"After\", step,\" training step(s), loss on test set is \", restoredSession.run([loss], feed_dict={x: testX, y_: testY}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (99, 246) for Tensor 'x-input:0', which has shape '(?, 245)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cdb45bf095e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstartTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMODEL_SAVE_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtwo_layers_model_fileName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLayer2Model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-04a1cddedbff>\u001b[0m in \u001b[0;36mstartTrain\u001b[1;34m(trainX, trainY, model_path, model_name, model)\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mhead\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtail\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" training step(s), loss on training batch is \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yw59785\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yw59785\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    973\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    976\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (99, 246) for Tensor 'x-input:0', which has shape '(?, 245)'"
     ]
    }
   ],
   "source": [
    "startTrain(trainX, trainY, MODEL_SAVE_PATH, two_layers_model_fileName, Layer2Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 初始化TensorFlow持久化类。\n",
    "# saver = tf.train.Saver()\n",
    "# config = tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "# sess = tf.Session(config = config)\n",
    "# with sess:\n",
    "#     tf.global_variables_initializer().run()\n",
    "        \n",
    "#     head = 0\n",
    "#     for i in range(TRAINING_STEPS):\n",
    "#         tail = head+BATCH_SIZE\n",
    "#         if tail > dataSize:\n",
    "#             xs = np.concatenate((trainX[head: BATCH_SIZE], trainX[0: tail-BATCH_SIZE]))\n",
    "#             ys = np.concatenate((trainY[head: BATCH_SIZE], trainY[0: tail-BATCH_SIZE]))\n",
    "#             head = tail - BATCH_SIZE\n",
    "#         else:\n",
    "#             xs, ys = trainX[head: head+BATCH_SIZE-1], trainY[head: head+BATCH_SIZE-1]\n",
    "#             head = tail\n",
    "            \n",
    "#         _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "#         if i % 1000 == 0:\n",
    "#             print(\"After\", step,\" training step(s), loss on training batch is \", loss_value)\n",
    "#         if i % 5000 == 0:\n",
    "#             saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "# restoredSession = tf.Session(config=config)\n",
    "# restoredSaver = tf.train.import_meta_graph(os.path.join(MODEL_SAVE_PATH, 'tianchi_model-34002.meta'))\n",
    "# restoredSaver.restore(restoredSession, os.path.join(MODEL_SAVE_PATH, 'tianchi_model-34002'))\n",
    "# restoredSession.run([loss], feed_dict={x: testX, y_: testY})\n",
    "# continueTrain(trainX, trainY, restoredSession, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
