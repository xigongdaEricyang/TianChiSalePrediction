{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import mnist_inference\n",
    "import Layer2Model\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100 \n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30001\n",
    "MOVING_AVERAGE_DECAY = 0.99 \n",
    "MODEL_SAVE_PATH = \"../model/{}\".format(pd.Timestamp.now())[:19]\n",
    "MODEL_NAME = \"tianchi_model\"\n",
    "two_layers_model_fileName = '2HiddenLayer'\n",
    "one_layers_model_fileName = '1HiddenLayer'\n",
    "TENSORBOARD_LOG = 'tensor_board'\n",
    "TrainDataFileName = 'withMax_trainData.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def startTrain(trainX, trainY, model_path, model_name, model):\n",
    "    dataSize = len(trainY)\n",
    "    \n",
    "    with tf.device('/device:GPU:0'):\n",
    "    # 定义输入输出placeholder。\n",
    "        x = tf.placeholder(tf.float32, [None, model.INPUT_NODE], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, model.OUTPUT_NODE], name='y-input')\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "        y = model.inference(x, regularizer)\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "#     cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "#     cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "        beginLoss = tf.sqrt(tf.reduce_mean(tf.pow(tf.subtract(y, y_), 2)))\n",
    "#     loss = beginLoss + tf.add_n(tf.get_collection('losses'))\n",
    "        loss = beginLoss\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            LEARNING_RATE_BASE,\n",
    "            global_step,\n",
    "            dataSize / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "            staircase=True)\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "        with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    config = tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "    with tf.Session(config = config) as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        head = 0\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            tail = head+BATCH_SIZE\n",
    "            if tail > dataSize:\n",
    "                xs = np.concatenate((trainX[head: BATCH_SIZE], trainX[0: tail-BATCH_SIZE]))\n",
    "                ys = np.concatenate((trainY[head: BATCH_SIZE], trainY[0: tail-BATCH_SIZE]))\n",
    "                head = tail - BATCH_SIZE\n",
    "            else:\n",
    "                xs, ys = trainX[head: head+BATCH_SIZE-1], trainY[head: head+BATCH_SIZE-1]\n",
    "                head = tail\n",
    "            \n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After\", step,\" training step(s), loss on training batch is \", loss_value)\n",
    "            if i % 10000 == 0:\n",
    "                saver.save(sess, os.path.join(model_path, model_name), global_step=global_step)\n",
    "                testLoss = sess.run([loss], feed_dict={x: testX, y_: testY})\n",
    "                print(\"After\", step,\" training step(s), loss on test set is \", testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle data, produce train and test input\n",
    "\n",
    "df = pd.read_csv(os.path.join('../CleanData/',TrainDataFileName))\n",
    "# random shift the df\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "normalizeColumns = ['compartment','TR','displacement','price_level','power','level_id',\n",
    "                    'cylinder_number','engine_torque','car_length','car_height','car_width','total_quality','equipment_quality',\n",
    "                    'rated_passenger','wheelbase','front_track','rear_track']\n",
    "leftDf = df.drop(normalizeColumns, axis =1 ).drop(['sale_quantity'], axis = 1)\n",
    "\n",
    "normalizeDf = df[normalizeColumns]\n",
    "normalizeDf = (normalizeDf-normalizeDf.min())/(normalizeDf.max()-normalizeDf.min())\n",
    "inputDf = pd.concat([leftDf, normalizeDf], axis = 1)\n",
    "inputX = inputDf.values\n",
    "resultArray = df['sale_quantity'].values\n",
    "inputY = resultArray.reshape((len(resultArray),1))\n",
    "trainX, testX, trainY, testY = train_test_split(inputX, inputY, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def continueTrain(trainX, trainY, sess, continue_steps):\n",
    "    head = 0\n",
    "    for i in range(continue_steps):\n",
    "        tail = head+BATCH_SIZE\n",
    "        if tail > dataSize:\n",
    "            xs = np.concatenate((trainX[head: BATCH_SIZE], trainX[0: tail-BATCH_SIZE]))\n",
    "            ys = np.concatenate((trainY[head: BATCH_SIZE], trainY[0: tail-BATCH_SIZE]))\n",
    "            head = tail - BATCH_SIZE\n",
    "        else:\n",
    "            xs, ys = trainX[head: head+BATCH_SIZE-1], trainY[head: head+BATCH_SIZE-1]\n",
    "            head = tail\n",
    "            \n",
    "        _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After\", step,\" training step(s), loss on training batch is \", loss_value)\n",
    "        if i % 10000 == 0:\n",
    "            saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "            testLoss = sess.run([loss], feed_dict={x: testX, y_: testY})\n",
    "            print(\"After\", step,\" training step(s), loss on test set is \", restoredSession.run([loss], feed_dict={x: testX, y_: testY}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1  training step(s), loss on training batch is  419.96\n",
      "After 1  training step(s), loss on test set is  [417.6713]\n",
      "After 1001  training step(s), loss on training batch is  353.274\n",
      "After 2001  training step(s), loss on training batch is  311.271\n",
      "After 3001  training step(s), loss on training batch is  285.99\n",
      "After 4001  training step(s), loss on training batch is  268.561\n",
      "After 5001  training step(s), loss on training batch is  255.727\n",
      "After 6001  training step(s), loss on training batch is  246.769\n",
      "After 7001  training step(s), loss on training batch is  241.195\n",
      "After 8001  training step(s), loss on training batch is  234.767\n",
      "After 9001  training step(s), loss on training batch is  230.813\n",
      "After 10001  training step(s), loss on training batch is  227.408\n",
      "After 10001  training step(s), loss on test set is  [212.97559]\n",
      "After 11001  training step(s), loss on training batch is  224.829\n",
      "After 12001  training step(s), loss on training batch is  222.763\n",
      "After 13001  training step(s), loss on training batch is  221.095\n",
      "After 14001  training step(s), loss on training batch is  219.761\n",
      "After 15001  training step(s), loss on training batch is  218.683\n",
      "After 16001  training step(s), loss on training batch is  217.803\n",
      "After 17001  training step(s), loss on training batch is  217.085\n",
      "After 18001  training step(s), loss on training batch is  216.5\n",
      "After 19001  training step(s), loss on training batch is  216.02\n",
      "After 20001  training step(s), loss on training batch is  215.629\n",
      "After 20001  training step(s), loss on test set is  [204.48814]\n",
      "After 21001  training step(s), loss on training batch is  215.308\n",
      "After 22001  training step(s), loss on training batch is  215.045\n",
      "After 23001  training step(s), loss on training batch is  214.829\n",
      "After 24001  training step(s), loss on training batch is  214.651\n",
      "After 25001  training step(s), loss on training batch is  214.505\n",
      "After 26001  training step(s), loss on training batch is  214.385\n",
      "After 27001  training step(s), loss on training batch is  214.287\n",
      "After 28001  training step(s), loss on training batch is  214.205\n",
      "After 29001  training step(s), loss on training batch is  214.139\n",
      "After 30001  training step(s), loss on training batch is  214.084\n",
      "After 30001  training step(s), loss on test set is  [203.49834]\n"
     ]
    }
   ],
   "source": [
    "new_path = os.path.join(MODEL_SAVE_PATH, one_layers_model_fileName)\n",
    "if not os.path.exists(new_path):\n",
    "    os.makedirs(new_path)\n",
    "startTrain(trainX, trainY, MODEL_SAVE_PATH, one_layers_model_fileName, mnist_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 初始化TensorFlow持久化类。\n",
    "# saver = tf.train.Saver()\n",
    "# config = tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "# sess = tf.Session(config = config)\n",
    "# with sess:\n",
    "#     tf.global_variables_initializer().run()\n",
    "        \n",
    "#     head = 0\n",
    "#     for i in range(TRAINING_STEPS):\n",
    "#         tail = head+BATCH_SIZE\n",
    "#         if tail > dataSize:\n",
    "#             xs = np.concatenate((trainX[head: BATCH_SIZE], trainX[0: tail-BATCH_SIZE]))\n",
    "#             ys = np.concatenate((trainY[head: BATCH_SIZE], trainY[0: tail-BATCH_SIZE]))\n",
    "#             head = tail - BATCH_SIZE\n",
    "#         else:\n",
    "#             xs, ys = trainX[head: head+BATCH_SIZE-1], trainY[head: head+BATCH_SIZE-1]\n",
    "#             head = tail\n",
    "            \n",
    "#         _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "#         if i % 1000 == 0:\n",
    "#             print(\"After\", step,\" training step(s), loss on training batch is \", loss_value)\n",
    "#         if i % 5000 == 0:\n",
    "#             saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "# restoredSession = tf.Session(config=config)\n",
    "# restoredSaver = tf.train.import_meta_graph(os.path.join(MODEL_SAVE_PATH, 'tianchi_model-34002.meta'))\n",
    "# restoredSaver.restore(restoredSession, os.path.join(MODEL_SAVE_PATH, 'tianchi_model-34002'))\n",
    "# restoredSession.run([loss], feed_dict={x: testX, y_: testY})\n",
    "# continueTrain(trainX, trainY, restoredSession, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
