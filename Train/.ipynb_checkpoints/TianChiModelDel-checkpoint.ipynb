{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import mnist_inference\n",
    "import Layer2Model\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100 \n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30001\n",
    "MOVING_AVERAGE_DECAY = 0.99 \n",
    "MODEL_SAVE_PATH = \"../model/{}\".format(pd.Timestamp.now())[:19]\n",
    "MODEL_NAME = \"tianchi_model\"\n",
    "two_layers_model_fileName = '2HiddenLayer'\n",
    "one_layers_model_fileName = '1HiddenLayer'\n",
    "TENSORBOARD_LOG = 'tensor_board'\n",
    "TrainDataFileName = 'withMax_trainData.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def startTrain(trainX, trainY, model_path, model_name, model):\n",
    "    dataSize = len(trainY)\n",
    "    \n",
    "    with tf.device('/device:GPU:0'):\n",
    "    # 定义输入输出placeholder。\n",
    "        x = tf.placeholder(tf.float32, [None, model.INPUT_NODE], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, model.OUTPUT_NODE], name='y-input')\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "        y = model.inference(x, regularizer)\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "#     cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "#     cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "        beginLoss = tf.sqrt(tf.reduce_mean(tf.pow(tf.subtract(y, y_), 2)))\n",
    "#     loss = beginLoss + tf.add_n(tf.get_collection('losses'))\n",
    "        loss = beginLoss\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            LEARNING_RATE_BASE,\n",
    "            global_step,\n",
    "            dataSize / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "            staircase=True)\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "        with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    config = tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "    with tf.Session(config = config) as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        head = 0\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            tail = head+BATCH_SIZE\n",
    "            if tail > dataSize:\n",
    "                xs = np.concatenate((trainX[head: BATCH_SIZE], trainX[0: tail-BATCH_SIZE]))\n",
    "                ys = np.concatenate((trainY[head: BATCH_SIZE], trainY[0: tail-BATCH_SIZE]))\n",
    "                head = tail - BATCH_SIZE\n",
    "            else:\n",
    "                xs, ys = trainX[head: head+BATCH_SIZE-1], trainY[head: head+BATCH_SIZE-1]\n",
    "                head = tail\n",
    "            \n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After\", step,\" training step(s), loss on training batch is \", loss_value)\n",
    "            if i % 10000 == 0:\n",
    "                saver.save(sess, os.path.join(model_path, model_name), global_step=global_step)\n",
    "                testLoss = sess.run([loss], feed_dict={x: testX, y_: testY})\n",
    "                print(\"After\", step,\" training step(s), loss on test set is \", testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle data, produce train and test input\n",
    "\n",
    "df = pd.read_csv(os.path.join('../CleanData/',TrainDataFileName))\n",
    "# random shift the df\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "normalizeColumns = ['compartment','TR','displacement','price_level','power','level_id',\n",
    "                    'cylinder_number','engine_torque','car_length','car_height','car_width','total_quality','equipment_quality',\n",
    "                    'rated_passenger','wheelbase','front_track','rear_track']\n",
    "leftDf = df.drop(normalizeColumns, axis =1 ).drop(['sale_quantity'], axis = 1)\n",
    "\n",
    "normalizeDf = df[normalizeColumns]\n",
    "normalizeDf = (normalizeDf-normalizeDf.min())/(normalizeDf.max()-normalizeDf.min())\n",
    "inputDf = pd.concat([leftDf, normalizeDf], axis = 1)\n",
    "inputX = inputDf.values\n",
    "resultArray = df['sale_quantity'].values\n",
    "inputY = resultArray.reshape((len(resultArray),1))\n",
    "trainX, testX, trainY, testY = train_test_split(inputX, inputY, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def continueTrain(trainX, trainY, sess, continue_steps):\n",
    "    head = 0\n",
    "    for i in range(continue_steps):\n",
    "        tail = head+BATCH_SIZE\n",
    "        if tail > dataSize:\n",
    "            xs = np.concatenate((trainX[head: BATCH_SIZE], trainX[0: tail-BATCH_SIZE]))\n",
    "            ys = np.concatenate((trainY[head: BATCH_SIZE], trainY[0: tail-BATCH_SIZE]))\n",
    "            head = tail - BATCH_SIZE\n",
    "        else:\n",
    "            xs, ys = trainX[head: head+BATCH_SIZE-1], trainY[head: head+BATCH_SIZE-1]\n",
    "            head = tail\n",
    "            \n",
    "        _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After\", step,\" training step(s), loss on training batch is \", loss_value)\n",
    "        if i % 10000 == 0:\n",
    "            saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "            testLoss = sess.run([loss], feed_dict={x: testX, y_: testY})\n",
    "            print(\"After\", step,\" training step(s), loss on test set is \", restoredSession.run([loss], feed_dict={x: testX, y_: testY}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1  training step(s), loss on training batch is  541.539\n",
      "After 1  training step(s), loss on test set is  [428.74216]\n",
      "After 1001  training step(s), loss on training batch is  456.979\n",
      "After 2001  training step(s), loss on training batch is  456.979\n",
      "After 3001  training step(s), loss on training batch is  456.979\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ac094be93648>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mstartTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMODEL_SAVE_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtwo_layers_model_fileName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLayer2Model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-04a1cddedbff>\u001b[0m in \u001b[0;36mstartTrain\u001b[1;34m(trainX, trainY, model_path, model_name, model)\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mhead\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtail\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" training step(s), loss on training batch is \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mg:\\python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_path = os.path.join(MODEL_SAVE_PATH, mnist_inference)\n",
    "if not os.path.exists(new_path):\n",
    "    os.makedirs(new_path)\n",
    "startTrain(trainX, trainY, MODEL_SAVE_PATH, two_layers_model_fileName, mnist_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 初始化TensorFlow持久化类。\n",
    "# saver = tf.train.Saver()\n",
    "# config = tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "# sess = tf.Session(config = config)\n",
    "# with sess:\n",
    "#     tf.global_variables_initializer().run()\n",
    "        \n",
    "#     head = 0\n",
    "#     for i in range(TRAINING_STEPS):\n",
    "#         tail = head+BATCH_SIZE\n",
    "#         if tail > dataSize:\n",
    "#             xs = np.concatenate((trainX[head: BATCH_SIZE], trainX[0: tail-BATCH_SIZE]))\n",
    "#             ys = np.concatenate((trainY[head: BATCH_SIZE], trainY[0: tail-BATCH_SIZE]))\n",
    "#             head = tail - BATCH_SIZE\n",
    "#         else:\n",
    "#             xs, ys = trainX[head: head+BATCH_SIZE-1], trainY[head: head+BATCH_SIZE-1]\n",
    "#             head = tail\n",
    "            \n",
    "#         _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "#         if i % 1000 == 0:\n",
    "#             print(\"After\", step,\" training step(s), loss on training batch is \", loss_value)\n",
    "#         if i % 5000 == 0:\n",
    "#             saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "# restoredSession = tf.Session(config=config)\n",
    "# restoredSaver = tf.train.import_meta_graph(os.path.join(MODEL_SAVE_PATH, 'tianchi_model-34002.meta'))\n",
    "# restoredSaver.restore(restoredSession, os.path.join(MODEL_SAVE_PATH, 'tianchi_model-34002'))\n",
    "# restoredSession.run([loss], feed_dict={x: testX, y_: testY})\n",
    "# continueTrain(trainX, trainY, restoredSession, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
